{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1085454,"sourceType":"datasetVersion","datasetId":605165},{"sourceId":2821319,"sourceType":"datasetVersion","datasetId":1725235},{"sourceId":9043954,"sourceType":"datasetVersion","datasetId":5452447},{"sourceId":9116196,"sourceType":"datasetVersion","datasetId":5502542},{"sourceId":9116286,"sourceType":"datasetVersion","datasetId":5502605}],"dockerImageVersionId":30748,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:02.954670Z","iopub.execute_input":"2024-08-11T06:57:02.955098Z","iopub.status.idle":"2024-08-11T06:57:07.466813Z","shell.execute_reply.started":"2024-08-11T06:57:02.955062Z","shell.execute_reply":"2024-08-11T06:57:07.465364Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-11 06:57:03.896000: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-11 06:57:03.896090: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-11 06:57:03.897950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def fetch(path):\n    file=open(path,\"r\")\n    content1=file.read()\n    file.close()\n    content1=(content1.split(\"\\n\"))\n    text=[]\n    label=[]\n    try:\n        for j,i in enumerate(content1):\n            text1,label1=i.split(\";\")\n            text.append(text1)\n            label.append(label1)\n    except Exception as e:\n        print(\"\")\n    return pd.DataFrame({'text': text, 'label': label})","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:07.468619Z","iopub.execute_input":"2024-08-11T06:57:07.469508Z","iopub.status.idle":"2024-08-11T06:57:07.477848Z","shell.execute_reply.started":"2024-08-11T06:57:07.469468Z","shell.execute_reply":"2024-08-11T06:57:07.476569Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_data=fetch(\"/kaggle/input/emotions-dataset-for-nlp/train.txt\")","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:07.479651Z","iopub.execute_input":"2024-08-11T06:57:07.480199Z","iopub.status.idle":"2024-08-11T06:57:07.527575Z","shell.execute_reply.started":"2024-08-11T06:57:07.480155Z","shell.execute_reply":"2024-08-11T06:57:07.526312Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = train_data[train_data['label'] != \"love\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:07.530988Z","iopub.execute_input":"2024-08-11T06:57:07.531422Z","iopub.status.idle":"2024-08-11T06:57:07.543360Z","shell.execute_reply.started":"2024-08-11T06:57:07.531374Z","shell.execute_reply":"2024-08-11T06:57:07.541968Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:07.546116Z","iopub.execute_input":"2024-08-11T06:57:07.547002Z","iopub.status.idle":"2024-08-11T06:57:07.565276Z","shell.execute_reply.started":"2024-08-11T06:57:07.546962Z","shell.execute_reply":"2024-08-11T06:57:07.563820Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                    text    label\n0                                i didnt feel humiliated  sadness\n1      i can go from feeling so hopeless to so damned...  sadness\n2       im grabbing a minute to post i feel greedy wrong    anger\n4                                   i am feeling grouchy    anger\n5      ive been feeling a little burdened lately wasn...  sadness\n...                                                  ...      ...\n15995  i just had a very brief time in the beanbag an...  sadness\n15996  i am now turning and i feel pathetic that i am...  sadness\n15997                     i feel strong and good overall      joy\n15998  i feel like this was such a rude comment and i...    anger\n15999  i know a lot but i feel so stupid because i ca...  sadness\n\n[14696 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i didnt feel humiliated</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i can go from feeling so hopeless to so damned...</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>im grabbing a minute to post i feel greedy wrong</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am feeling grouchy</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ive been feeling a little burdened lately wasn...</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15995</th>\n      <td>i just had a very brief time in the beanbag an...</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>15996</th>\n      <td>i am now turning and i feel pathetic that i am...</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>15997</th>\n      <td>i feel strong and good overall</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>15998</th>\n      <td>i feel like this was such a rude comment and i...</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>15999</th>\n      <td>i know a lot but i feel so stupid because i ca...</td>\n      <td>sadness</td>\n    </tr>\n  </tbody>\n</table>\n<p>14696 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_data2=pd.read_csv(\"/kaggle/input/go-emotions-google-emotions-dataset/go_emotions_dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:07.567271Z","iopub.execute_input":"2024-08-11T06:57:07.567669Z","iopub.status.idle":"2024-08-11T06:57:08.721282Z","shell.execute_reply.started":"2024-08-11T06:57:07.567635Z","shell.execute_reply":"2024-08-11T06:57:08.719284Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"set(train_data[\"label\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:08.723084Z","iopub.execute_input":"2024-08-11T06:57:08.724148Z","iopub.status.idle":"2024-08-11T06:57:08.734946Z","shell.execute_reply.started":"2024-08-11T06:57:08.724101Z","shell.execute_reply":"2024-08-11T06:57:08.733739Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'anger', 'fear', 'joy', 'sadness', 'surprise'}"},"metadata":{}}]},{"cell_type":"code","source":"train_data2=train_data2[[\"text\",\"disgust\",\"neutral\"]]","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:08.736592Z","iopub.execute_input":"2024-08-11T06:57:08.737213Z","iopub.status.idle":"2024-08-11T06:57:08.762661Z","shell.execute_reply.started":"2024-08-11T06:57:08.737154Z","shell.execute_reply":"2024-08-11T06:57:08.761569Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data2=train_data2[~((train_data2[\"disgust\"]==0) & (train_data2[\"neutral\"]==0))]","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:08.763898Z","iopub.execute_input":"2024-08-11T06:57:08.764253Z","iopub.status.idle":"2024-08-11T06:57:08.786570Z","shell.execute_reply.started":"2024-08-11T06:57:08.764224Z","shell.execute_reply":"2024-08-11T06:57:08.785376Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_data2","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:08.788392Z","iopub.execute_input":"2024-08-11T06:57:08.788770Z","iopub.status.idle":"2024-08-11T06:57:08.801774Z","shell.execute_reply.started":"2024-08-11T06:57:08.788738Z","shell.execute_reply":"2024-08-11T06:57:08.800562Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                                     text  disgust  neutral\n2          You do right, if you don't care then fuck 'em!        0        1\n4       [NAME] was nowhere near them, he was by the Fa...        0        1\n10      I have, and now that you mention it, I think t...        0        1\n12                                  BUT IT'S HER TURN! /s        0        1\n13                                           That is odd.        1        0\n...                                                   ...      ...      ...\n211209                                          OH YEAH!!        0        1\n211210     Let me give you a hint: THEY PLAY IN BOSTON!!!        0        1\n211211  to google cuz I wasn't alive back then but dam...        0        1\n211213                   Wow, she headlines two shows now        0        1\n211215  Youtube is my Wikipedia. Videos about any inte...        0        1\n\n[60599 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>disgust</th>\n      <th>neutral</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>You do right, if you don't care then fuck 'em!</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>I have, and now that you mention it, I think t...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>BUT IT'S HER TURN! /s</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>That is odd.</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>211209</th>\n      <td>OH YEAH!!</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>211210</th>\n      <td>Let me give you a hint: THEY PLAY IN BOSTON!!!</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>211211</th>\n      <td>to google cuz I wasn't alive back then but dam...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>211213</th>\n      <td>Wow, she headlines two shows now</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>211215</th>\n      <td>Youtube is my Wikipedia. Videos about any inte...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>60599 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"val_data=fetch(\"/kaggle/input/emotions-dataset-for-nlp/val.txt\")","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:08.803225Z","iopub.execute_input":"2024-08-11T06:57:08.803783Z","iopub.status.idle":"2024-08-11T06:57:08.816905Z","shell.execute_reply.started":"2024-08-11T06:57:08.803749Z","shell.execute_reply":"2024-08-11T06:57:08.815521Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"val_data= val_data[val_data['label'] != \"love\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:08.818352Z","iopub.execute_input":"2024-08-11T06:57:08.818732Z","iopub.status.idle":"2024-08-11T06:57:08.826950Z","shell.execute_reply.started":"2024-08-11T06:57:08.818699Z","shell.execute_reply":"2024-08-11T06:57:08.825756Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"test_data=fetch(\"/kaggle/input/emotions-dataset-for-nlp/test.txt\")","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:08.830654Z","iopub.execute_input":"2024-08-11T06:57:08.831467Z","iopub.status.idle":"2024-08-11T06:57:08.843360Z","shell.execute_reply.started":"2024-08-11T06:57:08.831415Z","shell.execute_reply":"2024-08-11T06:57:08.841980Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"test_data= test_data[test_data['label'] != \"love\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:09.109652Z","iopub.execute_input":"2024-08-11T06:57:09.110106Z","iopub.status.idle":"2024-08-11T06:57:09.117688Z","shell.execute_reply.started":"2024-08-11T06:57:09.110069Z","shell.execute_reply":"2024-08-11T06:57:09.116559Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade nltk\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:12.289063Z","iopub.execute_input":"2024-08-11T06:57:12.289773Z","iopub.status.idle":"2024-08-11T06:57:27.366465Z","shell.execute_reply.started":"2024-08-11T06:57:12.289707Z","shell.execute_reply":"2024-08-11T06:57:27.364536Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.2)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\n\n# Specify the directory for NLTK data\n# nltk.data.path.append('/usr/share/nltk_data')\n\n# Download necessary NLTK data\nnltk.download(\"wordnet\")\nnltk.download('stopwords')\nnltk.download('punkt_tab')\n\n# Import necessary modules\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport pandas as pd\n\n# Initialize the lemmatizer and stop words\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\n# Define a preprocessing function\ndef preprocess_text(text):\n    tokens = nltk.word_tokenize(text.lower())\n    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    return ' '.join(lemmatized_tokens)\n\n# Apply preprocessing to both columns\ntrain_data[\"text\"] = train_data['text'].apply(preprocess_text)\ntest_data['text'] = test_data['text'].apply(preprocess_text)\nval_data['text'] = val_data['text'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T06:57:27.369121Z","iopub.execute_input":"2024-08-11T06:57:27.369586Z","iopub.status.idle":"2024-08-11T07:09:10.798375Z","shell.execute_reply.started":"2024-08-11T06:57:27.369547Z","shell.execute_reply":"2024-08-11T07:09:10.796980Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data2['text'] = train_data2['text'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:09:10.800958Z","iopub.execute_input":"2024-08-11T07:09:10.802156Z","iopub.status.idle":"2024-08-11T07:47:33.944588Z","shell.execute_reply.started":"2024-08-11T07:09:10.802106Z","shell.execute_reply":"2024-08-11T07:47:33.943178Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_data2","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:47:33.946359Z","iopub.execute_input":"2024-08-11T07:47:33.946853Z","iopub.status.idle":"2024-08-11T07:47:33.962458Z","shell.execute_reply.started":"2024-08-11T07:47:33.946805Z","shell.execute_reply":"2024-08-11T07:47:33.961238Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                                     text  disgust  neutral\n2                                         right care fuck        0        1\n4                                name nowhere near falcon        0        1\n10                      mention think triggered nostalgia        0        1\n12                                                   turn        0        1\n13                                                    odd        1        0\n...                                                   ...      ...      ...\n211209                                            oh yeah        0        1\n211210                          let give hint play boston        0        1\n211211                 google cuz alive back damn like em        0        1\n211213                              wow headline two show        0        1\n211215  youtube wikipedia video interest read wikipedi...        0        1\n\n[60599 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>disgust</th>\n      <th>neutral</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>right care fuck</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>name nowhere near falcon</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>mention think triggered nostalgia</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>turn</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>odd</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>211209</th>\n      <td>oh yeah</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>211210</th>\n      <td>let give hint play boston</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>211211</th>\n      <td>google cuz alive back damn like em</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>211213</th>\n      <td>wow headline two show</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>211215</th>\n      <td>youtube wikipedia video interest read wikipedi...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>60599 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def fetch_embedding(path):\n    file = open(path, 'r', encoding = 'utf8')\n    content = file.readlines()\n    file.close()\n    embeddings = {}\n    for line in content:\n        line = line.split()\n        embeddings[line[0]] = np.array(line[1:], dtype = float)\n    return embeddings\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nembeddings=fetch_embedding(\"/kaggle/input/asdfghjk/glove.6B.100d.txt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_maxlen(data):\n    maxlen = 0\n    for i in range(len(data)):\n        maxlen = max(maxlen, len(data[i]))\n    return maxlen","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:47:33.964719Z","iopub.execute_input":"2024-08-11T07:47:33.965129Z","iopub.status.idle":"2024-08-11T07:47:33.975122Z","shell.execute_reply.started":"2024-08-11T07:47:33.965082Z","shell.execute_reply":"2024-08-11T07:47:33.973904Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data[\"text\"])\ntokenizer.fit_on_texts(test_data[\"text\"])\ntokenizer.fit_on_texts(val_data[\"text\"])\ntokenizer.fit_on_texts(train_data2[\"text\"])\nword2index = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:47:33.976671Z","iopub.execute_input":"2024-08-11T07:47:33.977134Z","iopub.status.idle":"2024-08-11T07:47:35.572424Z","shell.execute_reply.started":"2024-08-11T07:47:33.977092Z","shell.execute_reply":"2024-08-11T07:47:35.571388Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"len(word2index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tokens = tokenizer.texts_to_sequences(train_data[\"text\"])\ntest_tokens = tokenizer.texts_to_sequences(test_data[\"text\"])\nval_tokens = tokenizer.texts_to_sequences(val_data[\"text\"])\ntrain2_tokens = tokenizer.texts_to_sequences(train_data2[\"text\"])\n\n# print(Xtokens)\n# print(\"========================================\")\nmaxlen_train = get_maxlen(train_tokens)\nmaxlen_test = get_maxlen(test_tokens)\nmaxlen_val=get_maxlen(val_tokens)\nmaxlen_train2=get_maxlen(train2_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:47:35.574393Z","iopub.execute_input":"2024-08-11T07:47:35.574821Z","iopub.status.idle":"2024-08-11T07:47:37.155870Z","shell.execute_reply.started":"2024-08-11T07:47:35.574783Z","shell.execute_reply":"2024-08-11T07:47:37.154138Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import json\ntokenizer_json = tokenizer.to_json()\nwith open('tokenizer.json', 'w') as json_file:\n    json.dump(tokenizer_json, json_file)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:51:44.208764Z","iopub.execute_input":"2024-08-11T07:51:44.209278Z","iopub.status.idle":"2024-08-11T07:51:44.371659Z","shell.execute_reply.started":"2024-08-11T07:51:44.209241Z","shell.execute_reply":"2024-08-11T07:51:44.370292Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"maxlen=max(maxlen_val,maxlen_train,maxlen_test,maxlen_train2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(maxlen_val,maxlen_train,maxlen_test,maxlen_train2)\nprint(maxlen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data2['new_column'] = train_data2.apply(lambda row: 1 if row['disgust'] == 1 else (4 if row['neutral'] == 1 else 0), axis=1)\n\n# Remove the original columns\ntrain_data2.drop(columns=['disgust', 'neutral'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train=train_data[\"label\"]\nY_test=test_data[\"label\"]\nY_val=val_data[\"label\"]\nY_train2=train_data2[\"new_column\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mapping={ \"anger\":0,  \"disgust\":1,  \"fear\":2, \"joy\":3,  \"neutral\":4,  \"sadness\":5,  \"surprise\":6}\nY_train = [mapping[label] for label in Y_train]\nY_test = [mapping[label] for label in Y_test]\nY_val = [mapping[label] for label in Y_val]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train = to_categorical(Y_train)\nY_test = to_categorical(Y_test)\nY_val = to_categorical(Y_val)\nY_train2=to_categorical(Y_train2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_size = 100\nembedding_matrix = np.zeros((len(word2index)+1, embed_size))\n\nfor word, i in word2index.items():\n    if(word in embeddings.keys()):\n        embed_vector = embeddings[word]\n        embedding_matrix[i] = embed_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n# instantiate a distribution strategy\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.TPUStrategy(tpu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D,Input\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Define TPU strategy if needed\nX_train = train_tokens\nX_val = val_tokens\nX_test=test_tokens\nX_train2=train2_tokens\nmaxlen = max(max(len(seq) for seq in X_train), \n             max(len(seq) for seq in X_test), \n             max(len(seq) for seq in X_val),\n             max(len(seq) for seq in X_train2))\n\n# Pad sequences to ensure uniform length\nX_train_padded = pad_sequences(X_train, maxlen=maxlen, padding='post', truncating='post')\nX_test_padded = pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')\nX_val_padded = pad_sequences(X_val, maxlen=maxlen, padding='post', truncating='post')\nX_train2_padded = pad_sequences(X_train2, maxlen=maxlen, padding='post', truncating='post')\n\n# Convert to NumPy arrays\nX_train = np.array(X_train_padded)\nX_val = np.array(X_val_padded)\nX_test = np.array(X_test_padded)\nX_train2 = np.array(X_train2_padded)\n# Assuming Ytrain is already a NumPy array\ny_train = np.array(Y_train)\ny_test = np.array(Y_test)\ny_val = np.array(Y_val)\ny_train2 = np.array(Y_train2)\n\n\n\ny_train = np.argmax(Y_train, axis=-1)\ny_val = np.argmax(Y_val, axis=-1)\ny_test = np.argmax(Y_test, axis=-1)\ny_train2 = np.argmax(Y_train2, axis=-1)\n\ny_train_one_hot = to_categorical(y_train, num_classes=7)\ny_train2_one_hot = to_categorical(y_train2, num_classes=7)\ny_val_one_hot = to_categorical(y_val, num_classes=7)\ny_test_one_hot = to_categorical(y_test, num_classes=7)\n\n# Update the model compilation and training\n# Define Parameters\nembed_size = 100\n# embedding_matrix = np.random.rand(len(word2index) + 1, embed_size)  # Define this properly if needed\n\n# Define Title Neural Network\nwith tpu_strategy.scope():\n    inputs = Input(shape=(maxlen,))\n    x = Embedding(input_dim=len(word2index) + 1,\n                  output_dim=embed_size,\n                  weights=[embedding_matrix],\n                  input_length=maxlen,\n                  trainable=False)(inputs)\n    x = SpatialDropout1D(0.2)(x)\n    x = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(x)\n    x = Dense(50, activation='relu')(x)\n    outputs = Dense(7, activation='softmax')(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n\n    # Compile the model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train2.shape,y_train2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape,y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = np.vstack((X_train, X_train2)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yy_train=np.vstack((y_train_one_hot,y_train2_one_hot))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yy_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nhistory = model.fit(\n    train,  # Training data\n    yy_train,  # Training labels (one-hot encoded)\n    epochs=50,  # Number of epochs\n    batch_size=32,  # Batch size\n    validation_data=(X_val, y_val_one_hot),  # Use a validation split (one-hot encoded)\n    verbose=1  # Verbosity mode\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_one_hot.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath='model_for_text_emotion_updated(1).keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max',\n        verbose=1\n    )\n    history = model.fit(\n        train,  # Training data\n        yy_train,  # Training labels (one-hot encoded)\n        epochs=1,  # Number of epochs\n        batch_size=32,  # Batch size\n        validation_data=(X_val, y_val_one_hot),  # Use a validation split (one-hot encoded)\n        verbose=1,  # Verbosity mode\n        callbacks=[checkpoint_callback]\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    model.save(\"model_arch_for_text_emotion_updated.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_tokens\nX_val = val_tokens\nX_test=test_tokens\nmaxlen = max(max(len(seq) for seq in X_train), \n             max(len(seq) for seq in X_test), \n             max(len(seq) for seq in X_val))\n\n# Pad sequences to ensure uniform length\nX_train_padded = pad_sequences(X_train, maxlen=maxlen, padding='post', truncating='post')\nX_test_padded = pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')\nX_val_padded = pad_sequences(X_val, maxlen=maxlen, padding='post', truncating='post')\n\n# Convert to NumPy arrays\nX_train = np.array(X_train_padded)\nX_val = np.array(X_val_padded)\nX_test = np.array(X_test_padded)\n\n# Assuming Ytrain is already a NumPy array\ny_train = np.array(Y_train)\ny_test = np.array(Y_test)\ny_val = np.array(Y_val)\n\n\n\ny_train = np.argmax(Y_train, axis=-1)\ny_val = np.argmax(Y_val, axis=-1)\ny_test = np.argmax(Y_test, axis=-1)\n\n\ny_train_one_hot = to_categorical(y_train, num_classes=6)\ny_val_one_hot = to_categorical(y_val, num_classes=6)\ny_test_one_hot = to_categorical(y_test, num_classes=6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/model_architecture_for_text_emotion_updated_json.json', 'w') as json_file:\n    json_file.write(model.to_json())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n# model=load_model('/kaggle/input/zxcvbjdsx/model_arch_for_text_emotion.h5')\nmodel.load_weights('/kaggle/input/zxcvbjdsx/model_for_text_emotion_130.keras')\ntest_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot, verbose=1)\n\nprint(f'Test Loss: {test_loss}')\nprint(f'Test Accuracy: {test_accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}